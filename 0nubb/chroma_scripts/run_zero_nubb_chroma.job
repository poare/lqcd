#!/bin/bash
#ulimit -c unlimited
#SBATCH --job-name=0nubb_npr
#SBATCH --nodes=1
#SBATCH --time=144:00:00
#SBATCH --partition=long

source /usr/local/Modules/init/bash
# TOPDIR=/home/poare/0nubb/chroma_scripts

set -x

# cd $TOPDIR
source /data/d10b/users/poare/software/poare_build/env.sh

module load openmpi/3.1.5-gcc.7.3.0

export LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64:/opt/software/hdf5-1.10.6/lib:$LD_LIBRARY_PATH
export application=/data/d10b/users/poare/software/poare_build/install/chroma-double/bin/chroma

nCores=8
export GEOM="2 2 2 1"
export OMP_NUM_THREADS=8
cpus=$nodes
echo "=== Run MPI application on $nCores cores ==="

export QUDA_REORDER_LOCATION=GPU #GPU/CPU
export QUDA_RESOURCE_PATH="/data/d10a/projects/wdetmold_viz/"

ini_dir=/home/poare/lqcd/0nubb/chroma_scripts/input
ens=${ini_dir}/24I/ml0p01    # change this for each different ensemble

# path to configs is /data/d10b/ensembles/RBC/RBC_UKQCD_24_64/2+1f_24nt64_IWASAKI_b2.13_ls16_M1.8_ms0.04_mu0.005

for cfg_idx in `seq 0 9`
do
  for k in `seq 6`
  do
    ini_file=${ens}/cfg_${cfg_idx}/mom_${k}.ini.xml
    mpirun -np $nCores $application -i ${ini_file}  -qmpgeom ${GEOM}  -geom ${GEOM}
  done
done
