#!/bin/bash
#ulimit -c unlimited
#SBATCH --job-name=0nubb_npr
#SBATCH --nodes=1
#SBATCH --time=72:00:00
#SBATCH --partition=tough

# source /usr/local/Modules/init/bash

# TOPDIR=/home/poare/0nubb/chroma_scripts

set -x

# cd $TOPDIR
source /data/d10b/users/poare/software/poare_build/env.sh

module load openmpi/3.1.5-gcc.7.3.0

export LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64:/opt/software/hdf5-1.10.6/lib:$LD_LIBRARY_PATH
export application=/data/d10b/users/poare/software/poare_build/install/chroma-double/bin/chroma

nCores=8
export GEOM="2 2 2 1"
export OMP_NUM_THREADS=8
cpus=$nodes
echo "=== Run MPI application on $nCores cores ==="

export QUDA_REORDER_LOCATION=GPU #GPU/CPU
export QUDA_RESOURCE_PATH="/data/d10a/projects/wdetmold_viz/"

ini_dir=/home/lqcd/poare/lqcd/0nubb/chroma_scripts/input_glu
#ens=${ini_dir}/24I/ml0p01    # change this for each different ensemble
#ens=${ini_dir}/24I/ml0p005    # change this for each different ensemble
#ens=${ini_dir}/32I/ml0p006
ens=${ini_dir}/32I/ml0p004
#ens=${ini_dir}/32I/ml0p008

# path to configs is /data/d10b/ensembles/RBC/RBC_UKQCD_24_64/2+1f_24nt64_IWASAKI_b2.13_ls16_M1.8_ms0.04_mu0.005

# for cfg_idx in `seq 0 2`
for cfg_idx in 2 4
do
#cfg_idx=9
  k=3
  #for k in `seq 2 4`
  #do
    ini_file=${ens}/cfg_${cfg_idx}/mom_${k}.ini.xml
    mpirun -np $nCores $application -i ${ini_file}  -qmpgeom ${GEOM}  -geom ${GEOM}
  #done
done
