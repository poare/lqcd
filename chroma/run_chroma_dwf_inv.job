#!/bin/bash

##ulimit -c unlimited

#SBATCH --job-name=dwf_chroma
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=8
#SBATCH --partition=prod
#SBATCH -t 6:00:00

source /usr/local/Modules/init/bash

set -x

source /data/d10a/projects/nplqcd/wd_build/nojit/env.sh
module load openmpi/3.1.5-gcc.7.3.0

export LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64:$LD_LIBRARY_PATH #/home/agrebe/wombat/compute-node/install/llvm-75/lib/
export EXE=/data/d10a/projects/nplqcd/wd_build/nojit/install/nplqcd-double-master/bin/nplqcd

nCores=8
# GEOM="1 1 2 4"
export GEOM="2 2 2 1"
export OMP_NUM_THREADS=8
cpus=$nodes
echo "=== Run MPI application on $nCores cores ==="

export QUDA_REORDER_LOCATION=GPU #GPU/CPU
export QUDA_RESOURCE_PATH="/data/d10a/projects/wdetmold_viz/"

# FILE_IN=dwf_inv
# FILE_IN=dwf_UKQCD_inv
FILE_IN=dwf_UKQCD_QUDA_inv
# FILE_IN=chroma_dwf_nef_ex

home=/home/poare/lqcd/chroma
# output=${home}/output/dwf_test${SLURM_JOB_ID}.out.xml
output=/data/d10b/users/poare/0nubb/chroma_dwf_inversions/tests/dwf_test${SLURM_JOB_ID}.out.xml

# mpirun -np 8 $EXE -i ${home}/${FILE_IN}.ini.xml -o ${output} -geom ${GEOM}
mpirun -np $nCores $EXE -i ${home}/${FILE_IN}.ini.xml -o ${output}   -qmpgeom ${GEOM}  -geom ${GEOM}
